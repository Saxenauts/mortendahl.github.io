---
layout:     post
title:      "Secret Sharing, Part 2"
subtitle:   "Efficient Sharing with the Fast Fourier Transform"
date:       2017-06-05 12:00:00
header-img: "img/post-bg-03.jpg"
author:     "Morten Dahl"
twitter:    "mortendahlcs"
github:     "mortendahl"
linkedin:   "mortendahlcs"
---

<em><strong>TL;DR:</strong> efficient secret sharing requires fast polynomial evaluation and interpolation; here we go through what it takes to use the well-known Fast Fourier Transform for this.</em>

In the [first part](/2017/06/04/secret-sharing-part1/) we looked at Shamir's scheme, as well as its packed variant where several secrets are shared together. We saw that polynomials lie at the core of both schemes, and that implementation is basically a question of (partially) converting back and forth between two different representations of these. We also gave typical algorithms for doing this.

For this part we will look at somewhat more complex algorithms in an attempt to speed up the computations needed for generating shares. Specifically, we will implement and apply the Fast Fourier Transform, detailing all the essential steps. Performance measurements performed with [our Rust implementation](https://crates.io/crates/threshold-secret-sharing) shows that this yields orders of magnitude of efficiency improvements when either the number of shares or the number of secrets is high.

There is also an [associated Python notebook](https://github.com/mortendahl/privateml/blob/master/secret-sharing/Fast%20Fourier%20Transform.ipynb) to better see how the code samples fit together in the bigger picture.

# Polynomials

If we [look back](/2017/06/04/secret-sharing-part1/) at Shamir's scheme we see that it's all about polynomials: a random polynomial embedding the secret is sampled and the shares are taken as its values at a certain set of points.

```python
def shamir_share(secret):
    polynomial = sample_shamir_polynomial(secret)
    shares = [ evaluate_at_point(polynomial, p) for p in SHARE_POINTS ]
    return shares
```

The same goes for the packed variant, where several secrets are embedded in the sampled polynomial.


```python
def packed_share(secrets):
    polynomial = sample_packed_polynomial(secrets)
    shares = [ interpolate_at_point(polynomial, p) for p in SHARE_POINTS ]
    return shares
```

Notice however that they differ slightly in the second steps where the shares are computed: Shamir's scheme uses `evaluate_at_point` while the packed uses `interpolate_at_point`. The reason is that the sampled polynomial in the former case is in *coefficient representation* while in the latter it is in *point-value representation*.

Specifically, we often represent a polynomial `f` of degree `D == N-1` by a list of `N` coefficients `a0, ..., aD` such that `f(x) = (a0) + (a1 * x) + (a2 * x^2) + ... + (aD * x^D)`. This representation is convenient for many things, including efficiently evaluating the polynomial at a given point using e.g. [Horner's method](https://en.wikipedia.org/wiki/Horner%27s_method).

However, every such polynomial may also be represented by a set of `N` point-value pairs `(p1, v1), ..., (pN, vN)` where `vi == f(pi)` and all the `pi` are distinct. Evaluating the polynomial at a given point is still possible, yet now requires a more involved *interpolation* procedure that is typically less efficient, at least unless some preprocessing is performed. 

But the point-value representation also has the advantage that a degree `N-1` polynomial may be represented by *more than* `N` pairs. In this case there is some redundancy in the representation that we may for instance take advantage of in secret sharing (to reconstruct even if some shares are lost) and in coding theory (to decode correctly even if some errors occur during transmission).

The reason this works is that the result of interpolation on a point-value representation with `N` pairs is technically speaking defined with respect to the *least degree* polynomial `g` such that `g(pi) == vi` for all pairs in the set, which is [unique](https://en.wikipedia.org/wiki/Polynomial_interpolation#Uniqueness_of_the_interpolating_polynomial) and has at most degree `N-1`. This means that if two point-value representations are generated using the same polynomial `g` then interpolation on these will yield identical results, even when the two sets are of different sizes or use different points, since the least degree polynomial is the same. 

It is also why we can use the two representations somewhat interchangeably: if a point-value representation with `N` pairs where generated by a degree `N-1` polynomial `f`, then the unique least degree polynomial agreeing with these must be `f`. And since, for a fixed set of points, the set of coefficient lists of length `N` and the set of value lists of length `N` has the same cardinality (in our case `Q^N`) we must have a bijection between them.


# Fast Fourier Transform

With the two presentation of polynomials in mind we move on to how the [Fast Fourier Transform](https://en.wikipedia.org/wiki/Fast_Fourier_transform) (FFT) over finite fields -- <em>also known as the [Number Theoretic Transform](https://en.wikipedia.org/wiki/Discrete_Fourier_transform_(general)#Number-theoretic_transform) (NTT)</em> -- can be used to perform efficient conversion between them. And for me the best way of understanding this is through an example that can later be generalised into an algorithm.


## Walk-through example

Recall that all our computations happen in a prime field determined by a fixed prime `Q`, i.e. using the numbers `0, 1, ..., Q-1`. In this example we will use `Q = 433`, who's order `Q-1` is divisible by `4`: `Q-1 == 432 == 4 * k` with `k = 108`.

Assume then that we have a polynomial `A(x) = 1 + 2x + 3x^2 + 4x^3` over this field of with `N == 4` coefficients and degree `N-1 == 3`.

```python
A_coeffs = [ 1, 2, 3, 4 ]
```

Our goal is to turn this list of coefficients into a list of values `[ A(w0), A(w1), A(w2), A(w3) ]` of equal length, for points `w = [w0, w1, w2, w3]`.

The standard way of evaluating polynomials is of course one way of during this, which using Horner's rule can be done in a total of `Oh(N * N)` operations.

```python
A = lambda x: horner_evaluate(A_coeffs, x)

assert([ A(wi) for wi in w ] 
    == [ 10, 73, 431, 356 ])
```

But as we will see, the FFT allows us to do so more efficiently when the length is sufficiently large and the points are chosen with a certain structure; asymptotically we can then compute the values in `Oh(N * log N)` operations.

The first insight we need is that there is an alternative evaluation strategy that breaks `A` into two smaller polynomials. In particular, if we define polynomials `B(y) = 1 + 3y` and `C(y) = 2 + 4y` by taking every other coefficient from `A` then we have `A(x) == B(x * x) + x * C(x * x)`, which is straight-forward to verify by simply writing out the right-hand side.

This means that if we know values of `B(y)` and `C(y)` at the *squares* `v` of the `w` points, then we can use these to compute the values of `A(x)` at the `w` points using table look-ups: `A_values[i] = B_values[i] + w[i] * C_values[i]`.

```python
# split A into B and C
B_coeffs = A_coeffs[0::2] # == [ 1,    3,   ]
C_coeffs = A_coeffs[1::2] # == [    2,    4 ]

# square the w points
v = [ wi * wi % Q for wi in w ]

# somehow compute the values of B and C at the v points
# ...
assert( B_values == [ B(vi) for vi in v ] )
assert( C_values == [ C(vi) for vi in v ] )

# combine results into values of A at the w points
A_values = [ ( B_values[i] + w[i] * C_values[i] ) % Q for i,_ in enumerate(w) ]

assert( A_values == [ A(wi) for wi in w ] )
```

So far we haven't saved much, but the second insight fixes that: by picking the points `w` to be the elements of a subgroup of order 4, the `v` points used for `B` and `C` will form a subgroup of order 2 due to the squaring; hence, we will have `v[0] == v[2]` and `v[1] == v[3]` and so only need the first halves of `B_values` and `C_values` -- as such we have cut the subproblems in half!

Such subgroups are typically characterized by a generator, i.e. an element of the field that when raised to powers will take on exactly the values of the subgroup elements. Historically such generators are denoted by the omega symbol so let's follow that convention here as well.

```python
# generator of subgroup of order 4
omega4 = 179

w = [ pow(omega4, e, Q) for e in range(4) ]
assert( w == [1, 179, 432, 254] )
```

We shall return to how to find such generator below, but note that once we know one of order 4 then it's easy to find one of order 2: we simply square.

```python
# generator of subgroup of order 2
omega2 = omega4 * omega4 % Q

v = [ pow(omega2, e, Q) for e in range(2) ]
assert( v == [1, 432] )
```

As a quick test we may also check that the orders are indeed as claimed. Specifically, if we keep raising `omega4` to higher powers than we except to keep visiting the same four numbers, and likewise we expect to keep visiting the same two numbers for `omega2`.

```python
assert( [ pow(omega4, e, Q) for e in range(8) ] == [1, 179, 432, 254, 1, 179, 432, 254] )
assert( [ pow(omega2, e, Q) for e in range(8) ] == [1, 432,   1, 432, 1, 432,   1, 432] )
```

Using generators we also see that there is no need to explicitly calculate the lists `w` and `v` anymore as they are now implicitly defined by the generator. So, with these change we come back to our mission of computing the values of `A` at the points determined by the powers of `omega4`, which may then be done via `A_values[i] = B_values[i % 2] + pow(omega4, i, Q) * C_values[i % 2]`.

The third and final insight we need is that we can of course continue this process of diving the polynomial in half: to compute e.g. `B_values` we break `B` into two polynomials `D` and `E` and then follow the same procedure; in this case `D` and `E` will be simple constants but it works in the general case as well. The only requirement is that the length `N` is a power of 2 and that we can find a generator `omegaN` of a subgroup of this size.


## Algorithm for power of 2

Putting the above into an algorithm we get the following, where `omega` is assumed to be a generator of order `len(aX)`. Note that some typical optimizations are omitted for clarity (but see e.g. [the Python notebook](https://github.com/mortendahl/privateml/blob/master/secret-sharing/Fast%20Fourier%20Transform.ipynb)).

```python
def fft2_forward(A_coeffs, omega):
    if len(A_coeffs) == 1:
        return A_coeffs

    # split A into B and C such that A(x) = B(x^2) + x * C(x^2)
    B_coeffs = A_coeffs[0::2]
    C_coeffs = A_coeffs[1::2]
    # .. and recurse
    B_values = fft2_forward(B_coeffs, pow(omega, 2, Q))
    C_values = fft2_forward(C_coeffs, pow(omega, 2, Q))
        
    # combine subresults
    A_values = [0] * len(A_coeffs)
    Nhalf = len(A_coeffs) // 2
    for i in range(Nhalf):
        
        j = i
        x = pow(omega, j, Q)
        A_values[j] = (B_values[i] + x * C_values[i]) % Q
        
        j = i + Nhalf
        x = pow(omega, j, Q)
        A_values[j] = (B_values[i] + x * C_values[i]) % Q
      
    return A_values
```

With this procedure we may convert a polynomial in coefficient form to its point-value form, i.e. evaluate the polynomial, in `Oh(N * log N)` operations.  

The freedom we gave up to achieve this is that the number of coefficients `N` must now be a power of 2; but of course, some of the them may be zero so we are still free to choose the degree of the polynomial as we wish up to `N-1`. Also, we are no longer free to choose any set of evaluation points but have to choose a set with a certain subgroup structure.

Finally, it turns out that we can also use the above procedure to go in the opposite direction from point-value form to coefficient form, i.e. interpolate the least degree polynomial. We see that this is simply done by essentially treating the values as coefficients followed by a scaling, but won't go into the details here.

```python
def fft2_backward(A_values, omega):
    N_inv = inverse(len(A_values))
    A_coeffs = [ (a * N_inv) % Q for a in fft2_forward(A, inverse(omega)) ]
    return A_coeffs
```

Here however we may feel a stronger impact of the constraints implied by the FFT: while we can use zero coefficients to "patch up" the coefficient representation of a lower degree polynomial to make its length match our target length `N` but keeping its identity, we cannot simply add e.g. zero pairs to a point-value representation as it may change the implicit least degree polynomial; as we will see in the next blog post this has implications for our application to secret sharing if we also want to use the FFT for reconstruction.


## Algorithm for power of 3

Unsurprisingly there is nothing in the principles behind the FFT that means it will only work for powers of 2, and other bases can indeed be used as well. Luckily perhaps, since this plays a big part in our application to secret sharing as we will see below.

To adapt the FFT algorithm to powers of 3 we instead assume that the list of coefficients of `A` has such a length, and split it into three polynomials `B`, `C`, and `D` such that `A(x) = B(x^3) + x * C(x^3) + x^2 * D(x^3)`. As seen, we then use the cube of `omega` in the recursive calls instead of the square.

```python
def fft3_forward(A_coeffs, omega):
    if len(A_coeffs) == 1:
        return A_coeffs

    # split A into B, C, and D such that A(x) = B(x^3) + x * C(x^3) + x^2 * D(x^3)
    B_coeffs = A_coeffs[0::3]
    B_coeffs = A_coeffs[1::3]
    B_coeffs = A_coeffs[2::3]
    
    # apply recursively
    omega_cubed = pow(omega, 3, Q)
    B_values = fft3_forward(B_coeffs, omega_cubed)
    C_values = fft3_forward(B_coeffs, omega_cubed)
    D_values = fft3_forward(B_coeffs, omega_cubed)
        
    # combine subresults
    A_values = [0] * len(A_coeffs)
    Nthird = len(A_coeffs) // 3
    for i in range(Nthird):
        
        j  = i
        x  = pow(omega, j, Q)
        xx = pow(x, 2, Q)
        A_values[j] = (B_values[i] + x * C_values[i] + xx * D_values[i]) % Q
        
        j  = i + Nthird
        x  = pow(omega, j, Q)
        xx = pow(x, 2, Q)
        A_values[j] = (B_values[i] + x * C_values[i] + xx * D_values[i]) % Q
        
        j  = i + Nthird + Nthird
        x  = pow(omega, j, Q)
        xx = pow(x, 2, Q)
        A_values[j] = (B_values[i] + x * C_values[i] + xx * D_values[i]) % Q

    return A_values
```

And again we may go in the opposite direction and perform interpolation by simply treating the values as coefficients and performing a scaling.


## Optimizations

For easy of presentation we have omitted some typical optimizations here, perhaps most typically the fact that for powers of 2 we have the property that `pow(omega, i, Q) == -pow(omega, i + N/2, Q)`, meaning we can cut the number of exponentiations in `fft2` in half compared to what we did above.

More interestingly, the FFTs can be also run in-place and hence reusing the list in which the input is provided. This saves memory allocations and has a significant impact on performance. Likewise, we may gain improvements by switching to another number representation such as [Montgomery form](https://en.wikipedia.org/wiki/Montgomery_modular_multiplication). Both of these approaches are described in further detail [elsewhere](https://medium.com/snips-ai/optimizing-threshold-secret-sharing-c877901231e5).



# Application to Secret Sharing

We can now return to applying the FFT to the secret sharing schemes. As mentioned earlier, using this instead of the more traditional approaches makes most sense when the vectors we are dealing with are above a certain size, such as if we are generating many shares or sharing many secrets together. See [the Rust benchmarks](https://github.com/mortendahl/rust-threshold-secret-sharing) for more concrete numbers.

**notice that the order of our prime `Q == 433` from the walk-through example is also divided by a power of 3, `Q == 432 = 4 * 9 * k` with `k == 12`.**

## Shamir's scheme

In this scheme we can easily sample our polynomial directly in coefficient form, and hence the FFT is only relevant in the second step where we generate the shares. Concretely, we can directly sample the polynomial with a *small* number of coefficients, add extra zeros to get a *large* number of coefficients matching the number of shares we want, and then apply the forward FFT to turn this into a vector of values that we take as the shares.

```python
def shamir_share(secret):
    small_coeffs = [secret] + [random.randrange(Q) for _ in range(T)]
    large_coeffs = small_coeffs + [0] * (ORDER3 - len(small_coeffs))
    large_values = fft3_forward(large_coeffs)
    shares = large_values
    return shares
```

Note that we've used base 3 here to be consistent with the next scheme; base 2 would of course also have worked.


## Packed scheme

Recall that for this scheme it is less obvious how we can sample our polynomial directly in coefficient form, and hence we instead do so in point-value form. Specifically, we first use the backward FFT in base 2 to turn such a polynomial into coefficient form, and then as above use the forward FFT in base 3 to generate the shares. 

Note that we are hence dealing with two sets of points: those used during sampling, and those used during share generation -- and these cannot overlap! If they did the privacy guarantee would no longer be satisfied and some of the share values might literally equal some of the secrets.

Preventing this from happening is the reason we use two different (co-prime) bases: any two subgroups with a non-trivial overlap will have a common order divider (**todo make clearer**); so since the bases are co-prime they will only have the 1 in common. As such we are safe if we simply make sure to exclude the value at point 1 from being used. **todo recall what we saw in the walk-thought example, that the subgroup of order 4 contained the subgroup of order 2 -- this holds in general since for two power of 2 subgroups, one will contain the other!**

For sharing we first sample the values of the polynomial, fixing the value at point 1 to be a constant (in this case zero). Using the backward FFT we then turn this into a small vector of coefficients, which we then as in Shamir's scheme, extend with zero coefficients to get a larger vector of coefficients matching the forward FFT. Finally, since the first value obtained from this corresponds to point 1, and hence is the same as the constant used before, we remove it before returning the values as shares.

```python
def packed_share(secrets):
    small_values = [0] + secrets + [random.randrange(Q) for _ in range(T)]
    small_coeffs = fft2_backward(small_values)
    large_coeffs = small_coeffs + [0] * (ORDER3 - ORDER2)
    large_values = fft3_forward(large_coeffs)
    shares = large_values[1:]
    return shares
```

We will talk more about how to do efficient reconstruction in the next blog post, but note that if all the shares are known then the above sharing procedure can efficiently be run backwards by simply running the two FFTs in their opposite direction.

```python
def packed_reconstruct(shares):
    large_values = [0] + shares
    large_coeffs = fft3_backward(large_values)
    small_coeffs = large_coeffs[:ORDER2]
    small_values = fft2_forward(small_coeffs)
    secrets = small_values[1:K+1]
    return secrets
```

However this only works if all shares are known and correct: any loss or tampering will get in the way of using the FFT for reconstruction, unless we add an additional ingredient. Fixing this is the topic of the next blog post.


## Performance evaluation

(coming)


# Parameter Generation

Since there are no security implications in re-using a same fixed set of parameters across applications, parameter generation is perhaps less important compared to for instance key generation in encryption schemes. Nonetheless, one of the benefits of secret sharing schemes is their ability to avoid big expansion factors by using parameters tailored to the use case; concretely, to pick a field of just the right size. As such we shall now fill in this final piece of the puzzle and see how a set of parameters fitting with the FFTs used in the packed scheme can be generated.

Our main abstraction is the `generate_parameters` function which takes a desired minimum field size in bits, as well as the number of secrets `k` we which to packed together, the privacy threshold `t` we want, and the number `n` of shares to generate. Accounting for the value at point 1 that we are throwing away (see earlier) we must then have that `k + t + 1` is suitable for the base 2 FFT and that `n + 1` is suitable for the base 3 FFT -- which amounts to them being respectively a power of 2 or 3.

To then make sure that our field has a subgroup of those two sizes, we simply need to find a field whose order is divided by both sizes. Specifically, since we're considering only prime fields, we need to find a prime `q` such that its order `q-1` is divided by both sizes. Moreover, to also find the two subgroups we also need a generator `g` of the field.`

```python
def generate_parameters(min_bitsize, k, t, n):
    order2 = k + t + 1
    order3 = n + 1
    
    order_divisor = order2 * order3
    p, g = find_prime_field(min_bitsize, order_divisor)
    
    order = p - 1
    omega2 = pow(g, order // order2, p)
    omega3 = pow(g, order // order3, p)
    
    return p, omega2, omega3
```

Finding our `q` and `g` is done by `find_prime_field`, which works by first finding a prime of the right size and with the right order. To then also find the generator we need a piece of auxiliary information, namely the prime factors in the order.

```python
def find_prime_field(min_bitsize, order_divisor):
    p, order_prime_factors = find_prime(min_bitsize, order_divisor)
    g = find_generator(p, order_prime_factors)
    return p, g
```

The reason for this is that we can use the prime factors in the order to efficiently test whether an arbitrary candidate element in the field is in fact a generator. This follows from a well-known number theory as detailed in standard textbooks on the matter. **todo references, theorem statement?**

```python
def find_generator(prime, order_prime_factors):
    order = prime - 1
    for candidate in range(2, Q):
        for factor in order_prime_factors:
            exponent = order // factor
            if pow(candidate, exponent, Q) == 1:
                break
        else:
            return candidate
```

This leaves us with only a few question regarding prime numbers as explained next.


## Finding primes

To find a prime `q` with the desired structure (i.e. of a certain minimum size and whose order `q-1` has a given divisor) we may either do rejection sampling of primes until we hit one that satisfies our need, or we may construct it from smaller parts so that it by design fits with what we want. The latter seems more efficient so that is what we will do here.

Specifically, given `min_bitsize` and `order_divisor` we do rejection sampling over two values `k1` and `k2` until `q = k1 * k2 * order_divisor + 1` is a [probable prime](https://en.wikipedia.org/wiki/Probable_prime). The `k1` is used to ensure that the minimum size is met, and `k2` is used to give us a bit of wiggle room -- it can in principle be omitted, but empirical tests show that it doesn't have to be very large it give an efficiency boost, essentially at the expense of potentially overshooting the desired field size by a few bits. Finally, since we also need to know the prime factorization of `q - 1`, and since this in general is believed to be an [inherently slow process](https://en.wikipedia.org/wiki/Integer_factorization), we by construction ensure that `k1` is a prime so that we only have to factor `k2` and `order_divisor`, which we assume to be somewhat small.

```python
def find_prime(min_bitsize, order_divisor):
    while True:
        k1 = sample_prime(min_bitsize)
        for k2 in range(128):
            q = k1 * k2 * order_divisor + 1
            if is_prime(q):
                order_prime_factors  = [k1]
                order_prime_factors += prime_factor(k2)
                order_prime_factors += prime_factor(order_divisor)
                return q, order_prime_factors
```

Sampling primes are done using standard randomized primality tests.

```python
def sample_prime(bitsize):
    lower = 1 << (bitsize-1)
    upper = 1 << (bitsize)
    while True:
        candidate = random.randrange(lower, upper)
        if is_prime(candidate):
            return candidate
```

And factoring a number is done by simply trying a fixed set of all small primes in sequence; this will of course not work if the input is too large, but that is not likely to happen in real-world applications.

```python
def prime_factor(x):
    factors = []
    for prime in SMALL_PRIMES:
        if prime > x: break
        if x % prime == 0:
            factors.append(prime)
            x = remove_factor(x, prime)
    assert(x == 1)
    return factors
```

Putting these pieces together we end up with an efficient procedure for generating parameters for use with FFTs: finding large fields of size e.g. 128bits is a matter of milliseconds.


# Next Steps

While we have seen that the Fast Fourier Transform can be used to greatly speed up the sharing process, it has a serious limitation when it comes to speeding up the reconstruction process: in its current form it requires all shares to be present and untampered with. As such, for most application we will be forced to resort to the more traditional and slower approaches of Newton or Laplace interpolation.

In the next blog post we will look at a technique for also using the Fast Fourier Transform for reconstruction, using techniques from error correction codes to account for missing or faulty shares, yet get similar speedup benefits to what we achieved here.

